{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from time import sleep\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1200\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "num_features = 1\n",
    "window_size = seq_len//2\n",
    "step_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, seq_len):\n",
    "# Dataset is built similar to the tutorial: http://philipperemy.github.io/keras-stateful-lstm/\n",
    "# The input is a sequence with all zeros except for the value in the first index which can be 0 or 1\n",
    "# If the first index value is 0 then the output is 0, if it is 1 then the output is 1\n",
    "# The point of the dataset is to make sure that the hidden state propagated in the LSTM\n",
    "# is used to make predictions.\n",
    "    N_train = int(N*0.8)\n",
    "    X = np.zeros((N, seq_len), dtype=np.float32)\n",
    "    y = np.zeros((N,1), dtype=np.float32)\n",
    "    \n",
    "    indices = np.random.randint(0, N, size=N//2)\n",
    "    \n",
    "    X[indices, 0] = 1.0\n",
    "    y[indices, 0] = 1.0\n",
    "    \n",
    "    X_train, y_train = X[:N_train], y[:N_train]\n",
    "    X_test, y_test = X[N_train:], y[N_train:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_synthetic_test_data(N, seq_len):\n",
    "#     N_train = int(N*0.8)\n",
    "#     X = np.reshape(np.arange(0, N*seq_len, dtype=np.float32), (N, seq_len))\n",
    "#     y = np.reshape(np.arange(0,N, dtype=np.float32), (N,1))\n",
    "    \n",
    "    \n",
    "#     X_train, y_train = X[:N_train], y[:N_train]\n",
    "#     X_test, y_test = X[N_train:], y[N_train:]\n",
    "    \n",
    "#     return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = generate_data(N, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        N = X.shape[0]        \n",
    "        self.N = N\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = np.expand_dims(self.X[index], -1)\n",
    "        y = self.y[index]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequences(x, window_size):\n",
    "    # x : (batch_size, seq_len, features)\n",
    "    # Split a batch of sequence data to generate all the subsequences\n",
    "    # with a step size of 1 are returned.        \n",
    "    seq_len = x.shape[1]\n",
    "    assert(window_size <= seq_len)\n",
    "    num_steps = seq_len - window_size + 1\n",
    "    for i in range(num_steps):\n",
    "        yield x[:, i:i+window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_lstm_layers, batch_size):\n",
    "        super(Net, self).__init__()        \n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=num_hidden, \n",
    "                             num_layers=num_lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(num_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden = list()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        self.hidden = ((torch.zeros(self.num_lstm_layers, self.batch_size, self.num_hidden, dtype=torch.float32)), \n",
    "                       (torch.zeros(self.num_lstm_layers, self.batch_size, self.num_hidden, dtype=torch.float32)))\n",
    "    \n",
    "    def forward(self, batch_data):\n",
    "        self.hidden = [Variable(h.data) for h in self.hidden]\n",
    "        lstm_out, self.hidden = self.lstm1(batch_data, self.hidden)            \n",
    "        y_pred = self.sigmoid(self.fc1(lstm_out[:,-1]))\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "train_ds = SequenceDataset(X_train, y_train)\n",
    "train_dataloader = utils.DataLoader(train_ds, batch_size=batch_size, drop_last=True)\n",
    "net = Net(num_features=1, num_hidden=64, num_lstm_layers=1, batch_size=batch_size)\n",
    "learning_rate = 1e-3\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "test_ds = SequenceDataset(X_test, y_test)\n",
    "test_dataloader = utils.DataLoader(test_ds, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    tr_acc = 0\n",
    "    tr_loss = 0\n",
    "    total = 0\n",
    "    for batch_data, batch_labels in train_dataloader:\n",
    "        net.init_hidden()        \n",
    "        \n",
    "        for subsequence in get_subsequences(batch_data, window_size):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = net(subsequence)\n",
    "            loss = loss_fn(y_pred, batch_labels)\n",
    "            tr_loss += loss.item()\n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "\n",
    "            total += batch_size\n",
    "\n",
    "            y_pred = (y_pred > 0.5).type(torch.FloatTensor)\n",
    "            accuracy = (y_pred == batch_labels).sum().item()\n",
    "            tr_acc += accuracy\n",
    "        \n",
    "        \n",
    "    tr_loss = tr_loss/total\n",
    "    tr_acc = tr_acc/total\n",
    "        \n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total = 0\n",
    "    for batch_data, batch_labels in test_dataloader:\n",
    "        net.init_hidden()\n",
    "        for subsequence in get_subsequences(batch_data, window_size):\n",
    "            y_pred = net(batch_data)\n",
    "            loss = loss_fn(y_pred, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            y_pred = (y_pred > 0.5).type(torch.FloatTensor)\n",
    "            accuracy = (y_pred == batch_labels).sum().item()\n",
    "            test_acc += accuracy\n",
    "\n",
    "            total += batch_size\n",
    "    \n",
    "    test_loss = test_loss/total\n",
    "    test_acc = test_acc/total\n",
    "    print(\"Epoch \", epoch)\n",
    "    print(\"Training loss: \", tr_loss, \", Training accuracy: \", tr_acc)\n",
    "    print(\"Test loss: \", test_loss, \", Test accuracy: \", test_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=64, stateful=True, batch_input_shape=(batch_size, window_size, num_features)))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SequenceDataset(X_train, y_train)\n",
    "train_dataloader = utils.DataLoader(train_ds, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_ds = SequenceDataset(X_test, y_test)\n",
    "test_dataloader = utils.DataLoader(test_ds, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for batch_data, batch_label in train_dataloader:\n",
    "        model.reset_states()  \n",
    "        for subsequence in get_subsequences(batch_data, window_size):\n",
    "            loss, acc = model.train_on_batch(subsequence, batch_label)\n",
    "        \n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "    \n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for batch_data, batch_label in test_dataloader:\n",
    "        model.reset_states()\n",
    "        for subsequence in get_subsequences(batch_data, window_size):\n",
    "            loss, acc = model.test_on_batch(subsequence, batch_label)\n",
    "        \n",
    "            test_loss.append(loss)\n",
    "            test_acc.append(acc)        \n",
    "    print('Epoch {}'.format(epoch))\n",
    "    print('Training loss {}, Training accuracy : {}'.format(np.mean(train_loss), np.mean(train_acc)))\n",
    "    print('Test loss {}, Test accuracy {}'.format(np.mean(test_loss), np.mean(test_acc)))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
